library(glmtoolbox)
library(plyr)
library(tidyr)
library(ggplot2)
library(car)
library(randomForest)
library(factoextra)
library(caTools)
library(randomForest)
library(varImp)
library(gbm)
library(Metrics)
library(tidyverse)
library(corrplot)
library(MLmetrics)
library(Hmisc)
library(plotly)
library(htmlwidgets)
library(arm)
library(boot)
library(caret)
library(broom)
library(gridExtra)

###############
## Load data ##
###############

df1 <- read.csv("~/GoDeep/data/ACS_LODES_RAC_data.csv")
df.label1 <- read.csv("~/GoDeep/data/ACS_LODES_RAC_data_Labels.csv")
df1 <- df1[, colSums(abs(df1)) != 0] # remove the variables where all blocks contain zero

df2 <- read.csv("~/GoDeep/data/ACS_LODES_WAC_data.csv")
df.label2 <- read.csv("~/GoDeep/data/ACS_LODES_WAC_data_Labels.csv")
df2 <- df2[, colSums(abs(df2)) != 0] # remove the variables where all blocks contain zero

df.label1 <- df.label1[,c(1,4)]
colnames(df.label1) <- c("labelID", "labelName")
df.columns <- data.frame(labelID = colnames(df1))
colnames(df.label1) <- c("labelID")
df.label1 <- merge(df.columns ,df.label1, by="labelID")

df.label2 <- df.label2[,c(1,4)]
colnames(df.label2) <- c("labelID", "labelName")
df.columns <- data.frame(labelID = colnames(df2))
colnames(df.label2) <- c("labelID")
df.label2 <- merge(df.columns ,df.label2, by="labelID")

# Column labels
lab1 <- df.label1[,2]
lab1 <- gsub(",","",lab1 )
lab1  <- gsub(":","",lab1 )
lab1  <- gsub("\\$","",lab1 )
lab1  <- gsub(" ",".",lab1 )

lab2 <- df.label2[,2]
lab2 <- gsub(",","",lab2 )
lab2  <- gsub(":","",lab2 )
lab2  <- gsub("\\$","",lab2 )
lab2  <- gsub(" ",".",lab2 )

colnames(df1) <- c("GEOID", "DAC_indicator", lab1 )
df1$DAC_indicator <- as.factor(df1$DAC_indicator)

colnames(df2) <- c("GEOID", "DAC_indicator", lab2 )
df2$DAC_indicator <- as.factor(df2$DAC_indicator)

######################
## Data exploration ##
######################

# RAC

# Income
hist.data.frame(df1[,4:12])
hist.data.frame(df1[,13:19])

# Job category
hist.data.frame(df1[,21:29])
hist.data.frame(df1[,30:40])

# Total and employed
hist(df1[,20], xlab="Total employed", main="")
hist(df1[,3], xlab="Total population", main="")

# WAC

# Income
hist.data.frame(df2[,4:12])
hist.data.frame(df2[,13:19])

# Job category
hist.data.frame(df2[,21:29])
hist.data.frame(df2[,30:40])

# Total and employed
hist(df2[,20], xlab="Total employed", main="")
hist(df2[,3], xlab="Total population", main="")

###################
# Three datasets ##
###################

# Merge
df2 <- df2[,-(3:19)]
df <- merge(df1 ,df2, by="GEOID")
df$DAC_indicator  <- df$DAC_indicator.x
df <- df[ , !(names(df) %in% c("DAC_indicator.x","DAC_indicator.y"))]

df <- df1
#df <- df2

#####################
# Correlation plot ##
#####################

df[ , !(names(df) %in% c("DAC_indicator","GEOID"))] <- scale(df[ , !(names(df) %in% c("DAC_indicator","GEOID"))])
#df[,-c(1,2)] <- scale(df[,-c(1,2)])
#res <- cor(df[-c(1,2)],use = "complete.obs")
res <- cor(df[ , !(names(df) %in% c("DAC_indicator","GEOID"))],use = "complete.obs")
corrplot::corrplot(res,type = "upper", tl.col = "black", tl.cex = 0.5, tl.srt = 45)
         
###################################################
############# PCA #################################
###################################################

# PCA
df_pca <- df[ , !(names(df) %in% c("DAC_indicator","GEOID"))]
pca <- prcomp(df_pca, center = TRUE,scale. = TRUE)
summary(pca)
pca$rotation[,1:3]

# CLustering


# PCA patterns
barplot(sort(pca$rotation[,1]),las=2, cex.axis=0.5, cex.names=0.4)
barplot(sort(pca$rotation[,2]),las=2, cex.axis=0.5, cex.names=0.4)
barplot(sort(pca$rotation[,3]),las=2, cex.axis=0.5, cex.names=0.4)

# PCA plots
fviz_eig(pca)
fviz_pca_var(pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)     # Avoid text overlapping+
fviz_pca_biplot(pca, repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969")  # Individuals color

# Extract PCs
results <-as.data.frame(pca$x[,1:3])
names(results)<-c("PC1","PC2","PC3")
results

# Plot PCs
results %>%
  ggplot(aes(x=PC1,y=PC2))+
  geom_point(color="blue")

p <- plot_ly(x=results$PC1, y=results$PC2, z=results$PC3, type="scatter3d", mode="markers") %>%
  layout(scene = list(xaxis = list(title = 'PC1'),
                      yaxis = list(title = 'PC2'),
                      zaxis = list(title = 'PC3')), legend = list(title=list(text='<b> Experiment </b>')))
p
saveWidget(p, "PC_RAC.html", selfcontained = F, libdir = "lib")

###############
## Modeling ###
###############

# Data frame for modeling
df <- data.frame(cbind( DAC_indicator = df$DAC_indicator, results))
df$DAC_indicator <- as.factor(df$DAC_indicator)

# Remove leverage values from data
df <- df[-1396,] # Remove leverage values 
df <- df[-c(373,617),] # Remove leverage values 

# DAC vs PC box plots
grid.arrange(
  ggplot(df, aes(DAC_indicator, PC1)) + 
    geom_boxplot() +
    ggtitle("PC1 by DAC") + 
    ylab("PC1"),
  
  ggplot(df, aes(DAC_indicator, PC2)) + 
    geom_boxplot() +
    ggtitle("PC2 by DAC") + 
    ylab("PC2"),
  
  ggplot(df, aes(DAC_indicator, PC3)) + 
    geom_boxplot() +
    ggtitle("PC3 by DAC") + 
    ylab("PC3"),
  ncol = 3)

# Split to train and test test
set.seed(1200)  
split <- sample.split(row.names(df), SplitRatio = 0.7)
train <- subset(df, split == "TRUE")
test <- subset(df, split == "FALSE")
train <- df[split==TRUE,]

##########
## GLM ###
##########

set.seed(1200)  
fit.glm <- glm(DAC_indicator ~.,family=binomial(link='logit'),data=train)
#fit.glm <- glm(DAC_indicator ~ PC1 + PC2 + PC3 + I(PC1^2),family=binomial(link='logit'), data=train)
summary(fit.glm )
par(mfrow = c(2, 2))
plot(fit.glm)


# Bind the logit and tidying the data for plot
probabilities <- predict(fit.glm, type = "response")
mydata <- train %>%
  dplyr::select_if(is.numeric) 
predictors <- colnames(mydata)
mydata <- mydata %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")


# Leverage points
plot(fit.glm, which = 4, id.n = 3)
model.data <- augment(fit.glm) %>% 
  mutate(index = 1:n()) 
model.data %>% top_n(3, .cooksd)
ggplot(model.data, aes(index, .std.resid)) + 
  geom_point(aes(color = DAC_indicator), alpha = .5) +
  theme_bw()
model.data %>% 
  filter(abs(.std.resid) > 2.5)

# VIF
vif <- sqrt(car::vif(fit.glm))
vif

# McFadden's pseudo R2
with(summary(fit.glm), 1 - deviance/null.deviance)
# A rule of thumb that I found to be quite helpful is that a McFadden's pseudo R2 ranging from 0.2 to 0.4 i

# Residual plot for logistic
binnedplot(fitted(fit.glm), 
           residuals(fit.glm, type = "response"), 
           nclass = NULL, 
           xlab = "Expected Values", 
           ylab = "Average residual", 
           main = "Binned residual plot", 
           cex.pts = 0.8, 
           col.pts = 1, 
           col.int = "gray")




# Train error
prob <- predict(fit.glm, type = "response") # Predict the probability (p) 
predicted.classes.tr <- as.factor(ifelse(prob > 0.5, "1", "0"))
c.matrix.glm.tr = table(train$DAC_indicator, predicted.classes.tr)
c.matrix.glm.tr # Confusion Matrix for training
train.glm.single <- mean(predicted.classes.tr == train$DAC_indicator) #Train error
train.glm.single

# Test error
prob <- predict(fit.glm, type = "response", newdata = test[,-1])
predicted.classes.ts <- as.factor(ifelse(prob > 0.5, "1", "0"))
c.matrix.glm.ts = table(test$DAC_indicator, predicted.classes.ts)
c.matrix.glm.ts
test.glm.single <- mean(predicted.classes.ts == test$DAC_indicator)
test.glm.single

# 5-fold cross validation for train and test error
train.glm <- 1-cv.glm(train, fit.glm, K=5)$delta[2] 
train.glm
test.glm <- 1-cv.glm(test, fit.glm, K=5)$delta[2]
test.glm

# F1 score
confusionMatrix(predicted.classes.tr, train$DAC_indicator, mode = "everything", positive="1")
F1.glm <- F1_Score(train$DAC_indicator, predicted.classes.tr, positive = "0")
F1.glm 

#Precision = 780 / (780 + 130)
#Recall = 780/ (780 + 23) 
#F1= 2 * (Precision * Recall) / (Precision + Recall)
#F1
#0.910683


##########
### RF ###
##########

set.seed(1200)  # Setting seed
fit.rf  <-randomForest(DAC_indicator ~ .,data=train, importance=TRUE,ntree=1000)
fit.rf

# Evaluate variable importance
importance(fit.rf)
varImpPlot(fit.rf)

# Train error
predicted.classes.tr  <- predict(fit.rf, type = "response")
c.matrix.rf.tr <- table(train$DAC_indicator, predicted.classes.tr)
c.matrix.rf.tr
train.rf.single <- mean(predicted.classes.tr == train$DAC_indicator)
train.rf.single 

# Test error
predicted.classes.ts  <- predict(fit.rf, type = "response", newdata = test[,-1])
c.matrix.rf.ts = table(test$DAC_indicator, predicted.classes.ts)
c.matrix.rf.ts
test.rf.single <- mean(predicted.classes.ts == test$DAC_indicator)
test.rf.single

# RF with 5-fold cross validation
repeat_cv <- trainControl(method='repeatedcv', number=5)
set.seed(1200) 
fit.rf <- train(
  # Formula. We are using all variables to predict Species
  DAC_indicator~., 
  # Source of data; remove the Species variable
  data=train, 
  # `rf` method for random forest
  method='rf', 
  # Add repeated cross validation as trControl
  trControl=repeat_cv,
  # Accuracy to measure the performance of the model
  metric='Accuracy')

# Model
fit.rf$finalModel

# Get variable importance, and turn into a data frame
var_imp <- caret::varImp(fit.rf, scale=FALSE)$importance
var_imp <- data.frame(variables=row.names(var_imp), importance=var_imp$Overall)

## Create a plot of variable importance
var_imp %>%
  ## Sort the data by importance
  arrange(importance) %>%
  ## Create a ggplot object for aesthetic
  ggplot(aes(x=reorder(variables, importance), y=importance)) + 
  ## Plot the bar graph
  geom_bar(stat='identity') + 
  ## Flip the graph to make a horizontal bar plot
  coord_flip() + 
  ## Add x-axis label
  xlab('PCs') +
  ylab('Variable Importance') +
  ## Add a title
  labs(title='GBM variable importance') + 
  ## Some layout for the plot
  theme_minimal() + 
  theme(axis.text = element_text(size = 10), 
        axis.title = element_text(size = 15), 
        plot.title = element_text(size = 20), 
  )

# Train error
predicted.classes.tr <-predict(fit.rf$finalModel, type = "response")
c.matrix.rf.tr <- confusionMatrix(predicted.classes.tr , train$DAC_indicator)
c.matrix.rf.tr
train.rf <- fit.rf$results$Accuracy[1]
train.rf 

# Test error
predicted.classes.ts <- predict(
  ## Random forest object
  object=fit.rf, 
  ## Data to use for predictions; remove the Species
  newdata=test[, -1])
c.matrix.rf.ts <- confusionMatrix(predicted.classes.ts , test$DAC_indicator)
c.matrix.rf.ts
test.rf <- mean(predicted.classes.ts == test$DAC_indicator)
test.rf 


# F1 score
F1.rf <- F1_Score(train$DAC_indicator, predicted.classes.tr, positive = NULL)
F1.rf 

###########
### GBM ###
###########

#set.seed(1200) 
#fit.gbm <- gbm(DAC_indicator ~., data = train, distribution = "bernoulli",keep.data = TRUE,verbose = FALSE, n.cores = 1)
#perf_gbm1 = gbm.perf(fit.gbm, method = "cv") # Optimal number of trees with cross validation 

#Train error
#prob <- stats::predict(object = fit.gbm, n.trees = perf_gbm1,type = "response")
#predicted.classes.tr <- ifelse(prob > 0.5, "1", "0")
#c.matrix.gbm.tr <- table(train$DAC_indicator, predicted.classes.tr)
#c.matrix.gbm.tr
#train.gbm.single <- mean(predicted.classes.tr == train$DAC_indicator)
#train.gbm.single

#Test error
#prob <- stats::predict(object = fit.gbm, n.trees = perf_gbm1,type = "response", newdata = test[,-ncol(test)])
#predicted.classes.ts <- ifelse(prob > 0.5, "1", "0")
#c.matrix.gbm.ts = table(test$DAC_indicator, predicted.classes.ts)
#c.matrix.gbm.ts
#test.gbm<- mean(predicted.classes.ts != test$DAC_indicator)
#test.gbm

# GBM with 5-fold cross validation
set.seed(1200) 
fit.gbm <- train(
  # Formula. We are using all variables to predict Species
  DAC_indicator~., 
  # Source of data; remove the Species variable
  data=train, 
  distribution = "bernoulli",
  # `rf` method for random forest
  method="gbm", 
  # Add repeated cross validation as trControl
  trControl=repeat_cv,
  # Accuracy to measure the performance of the model
  metric='Accuracy')

# Model
fit.gbm

# Get variable importance, and turn into a data frame
var_imp <- caret::varImp(fit.gbm, scale=FALSE)$importance
var_imp <- data.frame(variables=row.names(var_imp), importance=var_imp$Overall)

## Create a plot of variable importance
var_imp %>%
  ## Sort the data by importance
  arrange(importance) %>%
  ## Create a ggplot object for aesthetic
  ggplot(aes(x=reorder(variables, importance), y=importance)) + 
  ## Plot the bar graph
  geom_bar(stat='identity') + 
  ## Flip the graph to make a horizontal bar plot
  coord_flip() + 
  ## Add x-axis label
  xlab('PCs') +
  ylab('Variable Importance') +
  ## Add a title
  labs(title='GBM variable importance') + 
  ## Some layout for the plot
  theme_minimal() + 
  theme(axis.text = element_text(size = 10), 
        axis.title = element_text(size = 15), 
        plot.title = element_text(size = 20), 
  )

# Train error
predicted.classes.tr <-predict(fit.gbm)
c.matrix.gbm.tr <- confusionMatrix(predicted.classes.tr , train$DAC_indicator)
c.matrix.gbm.tr
train.gbm <- fit.gbm$results$Accuracy[1]
train.gbm 

# Test error
predicted.classes.ts <- predict(
  ## Random forest object
  object=fit.gbm, 
  ## Data to use for predictions; remove the Species
  newdata=test[, -1])
c.matrix.gbm.ts <- confusionMatrix(predicted.classes.ts , test$DAC_indicator)
c.matrix.gbm.ts
test.gbm <- mean(predicted.classes.ts == test$DAC_indicator)
test.gbm 


# F1 score
F1.gbm <- F1_Score(train$DAC_indicator, predicted.classes.tr, positive = NULL)
F1.gbm 

###########
### CNN ###
###########

# GBM with 5-fold cross validation
set.seed(1200) 
fit.nn <- train(
  # Formula. We are using all variables to predict Species
  DAC_indicator~., 
  # Source of data; remove the Species variable
  data=train, 
  distribution = "bernoulli",
  # `rf` method for random forest
  method="nnet", 
  # Add repeated cross validation as trControl
  trControl=repeat_cv,
  # Accuracy to measure the performance of the model
  metric='Accuracy')

# Model
fit.nn

# Get variable importance, and turn into a data frame
var_imp <- caret::varImp(fit.nn, scale=FALSE)$importance
var_imp <- data.frame(variables=row.names(var_imp), importance=var_imp$Overall)

## Create a plot of variable importance
var_imp %>%
  ## Sort the data by importance
  arrange(importance) %>%
  ## Create a ggplot object for aesthetic
  ggplot(aes(x=reorder(variables, importance), y=importance)) + 
  ## Plot the bar graph
  geom_bar(stat='identity') + 
  ## Flip the graph to make a horizontal bar plot
  coord_flip() + 
  ## Add x-axis label
  xlab('PCs') +
  ylab('Variable Importance') +
  ## Add a title
  labs(title='GBM variable importance') + 
  ## Some layout for the plot
  theme_minimal() + 
  theme(axis.text = element_text(size = 10), 
        axis.title = element_text(size = 15), 
        plot.title = element_text(size = 20), 
  )

# Train error
predicted.classes.tr <-predict(fit.nn)
c.matrix.nn.tr <- confusionMatrix(predicted.classes.tr , train$DAC_indicator)
c.matrix.nn.tr
train.nn <- fit.nn$results$Accuracy[1]
train.nn 

# Test error
predicted.classes.ts <- predict(
  ## Random forest object
  object=fit.nn, 
  ## Data to use for predictions; remove the Species
  newdata=test[, -1])
c.matrix.nn.ts <- confusionMatrix(predicted.classes.ts , test$DAC_indicator)
c.matrix.nn.ts
test.nn <- mean(predicted.classes.ts == test$DAC_indicator)
test.nn 


# F1 score
F1.nn <- F1_Score(train$DAC_indicator, predicted.classes.tr, positive = NULL)
F1.nn 


##################
#### Results #####
##################

t(c.matrix.glm.tr)
c.matrix.rf.tr$table
c.matrix.gbm.tr$table
c.matrix.nn.tr$table

t(c.matrix.glm.ts)
c.matrix.rf.ts$table
c.matrix.gbm.ts$table
c.matrix.nn.ts$table

train.glm
train.rf
train.gbm
train.nn

test.glm
test.rf
test.gbm
test.nn

F1.glm
F1.rf
F1.gbm
F1.nn

# Variable importance plot

# The relative importance of predictor x is the sum of the squared improvements over all internal nodes 
# of the tree for which x was chosen as the partitioning variable. n ensembles, the improvement score for 
# each predictor is averaged across all the trees in the ensemble. 

vip::vip(fit.glm)
vip::vip(fit.rf$finalModel) 
vip::vip(fit.gbm$finalModel) 
vip::vip(fit.nn$finalModel) 


